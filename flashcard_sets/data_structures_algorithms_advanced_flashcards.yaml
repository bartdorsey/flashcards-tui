title: "Data Structures & Algorithms Advanced"
icon: "ðŸš€"
flashcards:
  - question: "What is a binary search tree (BST) and what are its properties?"
    answer: "A BST is a binary tree where for each node, all values in left subtree are smaller and all values in right subtree are larger"
    code_example: |
      class BST:
          def __init__(self, val):
              self.val = val
              self.left = None
              self.right = None
          
          def insert(self, val):
              if val < self.val:
                  if self.left:
                      self.left.insert(val)
                  else:
                      self.left = BST(val)
              else:
                  if self.right:
                      self.right.insert(val)
                  else:
                      self.right = BST(val)
      
      # BST Properties:
      # - Left subtree < root < right subtree
      # - In-order traversal gives sorted sequence
      # - Search/Insert/Delete: O(log n) average, O(n) worst
      # - No duplicate values (in basic implementation)

  - question: "What is a heap and what are the two main types?"
    answer: "A heap is a complete binary tree that satisfies the heap property. Max-heap: parent â‰¥ children, Min-heap: parent â‰¤ children"
    code_example: |
      import heapq
      
      # Min-heap (Python's default)
      min_heap = []
      heapq.heappush(min_heap, 10)
      heapq.heappush(min_heap, 5)
      heapq.heappush(min_heap, 15)
      heapq.heappush(min_heap, 3)
      
      # Extract minimum: O(log n)
      minimum = heapq.heappop(min_heap)  # Returns 3
      
      # Max-heap (negate values)
      max_heap = []
      heapq.heappush(max_heap, -10)
      heapq.heappush(max_heap, -5)
      maximum = -heapq.heappop(max_heap)  # Returns 10
      
      # Heap operations:
      # Insert: O(log n), Extract-min/max: O(log n)
      # Peek: O(1), Build heap: O(n)
      # Used in: Priority queues, heap sort

  - question: "What is merge sort and how does it work?"
    answer: "Merge sort is a divide-and-conquer algorithm that recursively divides array into halves, sorts them, and merges them back"
    code_example: |
      def merge_sort(arr):
          if len(arr) <= 1:
              return arr
          
          # Divide
          mid = len(arr) // 2
          left = merge_sort(arr[:mid])
          right = merge_sort(arr[mid:])
          
          # Conquer (merge)
          return merge(left, right)
      
      def merge(left, right):
          result = []
          i = j = 0
          
          # Compare elements and merge
          while i < len(left) and j < len(right):
              if left[i] <= right[j]:
                  result.append(left[i])
                  i += 1
              else:
                  result.append(right[j])
                  j += 1
          
          # Add remaining elements
          result.extend(left[i:])
          result.extend(right[j:])
          return result
      
      # Time: O(n log n) - always stable
      # Space: O(n) - needs extra space

  - question: "What is quick sort and how does it work?"
    answer: "Quick sort picks a pivot, partitions array around pivot, then recursively sorts subarrays"
    code_example: |
      def quick_sort(arr, low=0, high=None):
          if high is None:
              high = len(arr) - 1
          
          if low < high:
              # Partition and get pivot index
              pi = partition(arr, low, high)
              
              # Recursively sort elements before and after partition
              quick_sort(arr, low, pi - 1)
              quick_sort(arr, pi + 1, high)
      
      def partition(arr, low, high):
          # Choose last element as pivot
          pivot = arr[high]
          i = low - 1  # Index of smaller element
          
          for j in range(low, high):
              if arr[j] <= pivot:
                  i += 1
                  arr[i], arr[j] = arr[j], arr[i]
          
          arr[i + 1], arr[high] = arr[high], arr[i + 1]
          return i + 1
      
      # Time: O(n log n) average, O(nÂ²) worst
      # Space: O(log n) - in-place but recursive calls
      # Very fast in practice

  - question: "What is a graph and what are the main ways to represent it?"
    answer: "A graph is a collection of vertices (nodes) connected by edges. Can be represented using adjacency matrix or adjacency list"
    code_example: |
      # Adjacency List representation
      graph_list = {
          'A': ['B', 'C'],
          'B': ['A', 'D', 'E'],
          'C': ['A', 'F'],
          'D': ['B'],
          'E': ['B', 'F'],
          'F': ['C', 'E']
      }
      
      # Adjacency Matrix representation
      # For vertices A=0, B=1, C=2, D=3, E=4, F=5
      graph_matrix = [
          [0, 1, 1, 0, 0, 0],  # A connects to B, C
          [1, 0, 0, 1, 1, 0],  # B connects to A, D, E
          [1, 0, 0, 0, 0, 1],  # C connects to A, F
          [0, 1, 0, 0, 0, 0],  # D connects to B
          [0, 1, 0, 0, 0, 1],  # E connects to B, F
          [0, 0, 1, 0, 1, 0]   # F connects to C, E
      ]
      
      # Adjacency List: O(V + E) space, faster for sparse graphs
      # Adjacency Matrix: O(VÂ²) space, faster edge lookup

  - question: "What is depth-first search (DFS) in graphs and how do you implement it?"
    answer: "DFS explores as far as possible along each branch before backtracking, using a stack (or recursion)"
    code_example: |
      # Recursive DFS
      def dfs_recursive(graph, start, visited=None):
          if visited is None:
              visited = set()
          
          visited.add(start)
          print(start)  # Process node
          
          for neighbor in graph[start]:
              if neighbor not in visited:
                  dfs_recursive(graph, neighbor, visited)
      
      # Iterative DFS using stack
      def dfs_iterative(graph, start):
          visited = set()
          stack = [start]
          
          while stack:
              node = stack.pop()
              if node not in visited:
                  visited.add(node)
                  print(node)  # Process node
                  
                  # Add neighbors to stack (reverse order for same traversal)
                  for neighbor in reversed(graph[node]):
                      if neighbor not in visited:
                          stack.append(neighbor)
      
      # Time: O(V + E), Space: O(V)
      # Used for: Topological sort, finding connected components

  - question: "What is breadth-first search (BFS) in graphs and how do you implement it?"
    answer: "BFS explores all neighbors at current depth before going to next depth level, using a queue"
    code_example: |
      from collections import deque
      
      def bfs(graph, start):
          visited = set()
          queue = deque([start])
          visited.add(start)
          
          while queue:
              node = queue.popleft()
              print(node)  # Process node
              
              for neighbor in graph[node]:
                  if neighbor not in visited:
                      visited.add(neighbor)
                      queue.append(neighbor)
      
      # BFS with level tracking
      def bfs_with_levels(graph, start):
          visited = set([start])
          queue = deque([(start, 0)])  # (node, level)
          
          while queue:
              node, level = queue.popleft()
              print(f"Node {node} at level {level}")
              
              for neighbor in graph[node]:
                  if neighbor not in visited:
                      visited.add(neighbor)
                      queue.append((neighbor, level + 1))
      
      # Time: O(V + E), Space: O(V)
      # Used for: Shortest path, level-order traversal

  - question: "What is dynamic programming and what are its key principles?"
    answer: "Dynamic programming solves complex problems by breaking them into subproblems and storing results to avoid redundant calculations"
    code_example: |
      # Fibonacci without DP: O(2^n) - exponential!
      def fib_naive(n):
          if n <= 1:
              return n
          return fib_naive(n-1) + fib_naive(n-2)
      
      # Fibonacci with memoization (top-down DP): O(n)
      def fib_memo(n, memo={}):
          if n in memo:
              return memo[n]
          if n <= 1:
              return n
          memo[n] = fib_memo(n-1, memo) + fib_memo(n-2, memo)
          return memo[n]
      
      # Fibonacci with tabulation (bottom-up DP): O(n)
      def fib_dp(n):
          if n <= 1:
              return n
          dp = [0] * (n + 1)
          dp[1] = 1
          for i in range(2, n + 1):
              dp[i] = dp[i-1] + dp[i-2]
          return dp[n]
      
      # DP principles:
      # 1. Optimal substructure
      # 2. Overlapping subproblems
      # 3. Memoization or tabulation

  - question: "What is the knapsack problem and how do you solve it with DP?"
    answer: "The 0/1 knapsack problem: given items with weights and values, maximize value in knapsack with weight capacity"
    code_example: |
      def knapsack(weights, values, capacity):
          n = len(weights)
          # dp[i][w] = max value using first i items with weight limit w
          dp = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]
          
          for i in range(1, n + 1):
              for w in range(1, capacity + 1):
                  # If current item's weight > capacity, skip it
                  if weights[i-1] > w:
                      dp[i][w] = dp[i-1][w]
                  else:
                      # Max of: include item or exclude item
                      include = values[i-1] + dp[i-1][w - weights[i-1]]
                      exclude = dp[i-1][w]
                      dp[i][w] = max(include, exclude)
          
          return dp[n][capacity]
      
      # Example:
      weights = [1, 3, 4, 5]
      values = [1, 4, 5, 7]
      capacity = 7
      max_value = knapsack(weights, values, capacity)  # Returns 9
      
      # Time: O(n * capacity), Space: O(n * capacity)

  - question: "What is Dijkstra's algorithm and when do you use it?"
    answer: "Dijkstra's algorithm finds shortest paths from a source vertex to all other vertices in a weighted graph with non-negative edges"
    code_example: |
      import heapq
      
      def dijkstra(graph, start):
          # Distance from start to all nodes
          distances = {node: float('infinity') for node in graph}
          distances[start] = 0
          
          # Priority queue: (distance, node)
          pq = [(0, start)]
          visited = set()
          
          while pq:
              current_dist, current = heapq.heappop(pq)
              
              if current in visited:
                  continue
              visited.add(current)
              
              # Check neighbors
              for neighbor, weight in graph[current].items():
                  distance = current_dist + weight
                  
                  # If shorter path found, update
                  if distance < distances[neighbor]:
                      distances[neighbor] = distance
                      heapq.heappush(pq, (distance, neighbor))
          
          return distances
      
      # Graph representation: {node: {neighbor: weight}}
      graph = {
          'A': {'B': 1, 'C': 4},
          'B': {'A': 1, 'C': 2, 'D': 5},
          'C': {'A': 4, 'B': 2, 'D': 1},
          'D': {'B': 5, 'C': 1}
      }
      
      # Time: O((V + E) log V), Space: O(V)

  - question: "What is a trie (prefix tree) and what are its use cases?"
    answer: "A trie is a tree data structure used to store strings where each path from root to leaf represents a word"
    code_example: |
      class TrieNode:
          def __init__(self):
              self.children = {}  # Dictionary of children
              self.is_end_word = False  # Marks end of a word
      
      class Trie:
          def __init__(self):
              self.root = TrieNode()
          
          def insert(self, word):
              node = self.root
              for char in word:
                  if char not in node.children:
                      node.children[char] = TrieNode()
                  node = node.children[char]
              node.is_end_word = True
          
          def search(self, word):
              node = self.root
              for char in word:
                  if char not in node.children:
                      return False
                  node = node.children[char]
              return node.is_end_word
          
          def starts_with(self, prefix):
              node = self.root
              for char in prefix:
                  if char not in node.children:
                      return False
                  node = node.children[char]
              return True
      
      # Use cases: Autocomplete, spell checkers, IP routing
      # Time: O(m) for insert/search where m = word length

  - question: "What is a union-find (disjoint set) data structure?"
    answer: "Union-Find efficiently tracks connected components in a graph with union and find operations"
    code_example: |
      class UnionFind:
          def __init__(self, n):
              self.parent = list(range(n))  # Each node is its own parent
              self.rank = [0] * n           # Height of tree rooted at i
          
          def find(self, x):
              # Path compression: make x point directly to root
              if self.parent[x] != x:
                  self.parent[x] = self.find(self.parent[x])
              return self.parent[x]
          
          def union(self, x, y):
              root_x = self.find(x)
              root_y = self.find(y)
              
              if root_x != root_y:
                  # Union by rank: attach smaller tree to larger
                  if self.rank[root_x] < self.rank[root_y]:
                      self.parent[root_x] = root_y
                  elif self.rank[root_x] > self.rank[root_y]:
                      self.parent[root_y] = root_x
                  else:
                      self.parent[root_y] = root_x
                      self.rank[root_x] += 1
          
          def connected(self, x, y):
              return self.find(x) == self.find(y)
      
      # Time: O(Î±(n)) amortized, where Î± is inverse Ackermann
      # Use cases: Kruskal's MST, cycle detection, connected components

  - question: "What is topological sorting and when is it used?"
    answer: "Topological sorting is a linear ordering of vertices in a DAG such that for every edge uâ†’v, u comes before v"
    code_example: |
      from collections import defaultdict, deque
      
      # Kahn's algorithm using BFS
      def topological_sort_bfs(graph, num_vertices):
          # Calculate in-degrees
          in_degree = [0] * num_vertices
          for u in graph:
              for v in graph[u]:
                  in_degree[v] += 1
          
          # Queue of vertices with no incoming edges
          queue = deque([i for i in range(num_vertices) if in_degree[i] == 0])
          result = []
          
          while queue:
              u = queue.popleft()
              result.append(u)
              
              # Remove u and decrease in-degree of neighbors
              for v in graph[u]:
                  in_degree[v] -= 1
                  if in_degree[v] == 0:
                      queue.append(v)
          
          # Check for cycle
          return result if len(result) == num_vertices else []
      
      # DFS approach
      def topological_sort_dfs(graph):
          visited = set()
          stack = []
          
          def dfs(v):
              visited.add(v)
              for neighbor in graph[v]:
                  if neighbor not in visited:
                      dfs(neighbor)
              stack.append(v)
          
          for vertex in graph:
              if vertex not in visited:
                  dfs(vertex)
          
          return stack[::-1]  # Reverse for correct order
      
      # Use cases: Course scheduling, build systems, task dependencies

  - question: "What is the difference between backtracking and dynamic programming?"
    answer: "Backtracking explores all possibilities by undoing choices when they lead to dead ends. DP stores solutions to avoid recomputation"
    code_example: |
      # BACKTRACKING: N-Queens problem
      def solve_n_queens(n):
          def is_safe(board, row, col):
              # Check column and diagonals
              for i in range(row):
                  if (board[i] == col or 
                      board[i] - i == col - row or 
                      board[i] + i == col + row):
                      return False
              return True
          
          def backtrack(board, row):
              if row == n:
                  return [board[:]]  # Found solution
              
              solutions = []
              for col in range(n):
                  if is_safe(board, row, col):
                      board[row] = col        # Make choice
                      solutions.extend(backtrack(board, row + 1))
                      board[row] = -1         # Undo choice (backtrack)
              return solutions
          
          return backtrack([-1] * n, 0)
      
      # DYNAMIC PROGRAMMING: Longest Common Subsequence
      def lcs(s1, s2):
          m, n = len(s1), len(s2)
          dp = [[0] * (n + 1) for _ in range(m + 1)]
          
          for i in range(1, m + 1):
              for j in range(1, n + 1):
                  if s1[i-1] == s2[j-1]:
                      dp[i][j] = dp[i-1][j-1] + 1
                  else:
                      dp[i][j] = max(dp[i-1][j], dp[i][j-1])
          
          return dp[m][n]
      
      # Backtracking: Explores all paths, can undo
      # DP: Stores optimal solutions, builds up answer

  - question: "What are AVL trees and how do they maintain balance?"
    answer: "AVL trees are self-balancing BSTs where height difference between left and right subtrees is at most 1"
    code_example: |
      class AVLNode:
          def __init__(self, val):
              self.val = val
              self.left = None
              self.right = None
              self.height = 1
      
      class AVLTree:
          def get_height(self, node):
              return node.height if node else 0
          
          def get_balance(self, node):
              return self.get_height(node.left) - self.get_height(node.right)
          
          def rotate_right(self, y):
              x = y.left
              T2 = x.right
              
              # Perform rotation
              x.right = y
              y.left = T2
              
              # Update heights
              y.height = 1 + max(self.get_height(y.left), self.get_height(y.right))
              x.height = 1 + max(self.get_height(x.left), self.get_height(x.right))
              
              return x  # New root
          
          def rotate_left(self, x):
              y = x.right
              T2 = y.left
              
              y.left = x
              x.right = T2
              
              x.height = 1 + max(self.get_height(x.left), self.get_height(x.right))
              y.height = 1 + max(self.get_height(y.left), self.get_height(y.right))
              
              return y
          
          def insert(self, root, val):
              # Normal BST insertion
              if not root:
                  return AVLNode(val)
              
              if val < root.val:
                  root.left = self.insert(root.left, val)
              else:
                  root.right = self.insert(root.right, val)
              
              # Update height and rebalance
              root.height = 1 + max(self.get_height(root.left), self.get_height(root.right))
              balance = self.get_balance(root)
              
              # Left Heavy
              if balance > 1:
                  if val < root.left.val:  # Left-Left
                      return self.rotate_right(root)
                  else:  # Left-Right
                      root.left = self.rotate_left(root.left)
                      return self.rotate_right(root)
              
              # Right Heavy
              if balance < -1:
                  if val > root.right.val:  # Right-Right
                      return self.rotate_left(root)
                  else:  # Right-Left
                      root.right = self.rotate_right(root.right)
                      return self.rotate_left(root)
              
              return root
      
      # Guarantees O(log n) for all operations
      # Four rotation cases: LL, LR, RR, RL